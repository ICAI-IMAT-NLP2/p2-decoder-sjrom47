{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Practice 2: Generating names with a decoder-only transformer\n",
    "\n",
    "In this notebook, we will generate new names using a Transformer decoder model. This simple text generation task captures the essential components of language modeling, applied to a small, manageable dataset.\n",
    "\n",
    "More specifically, you will be training autoregressive, character-level, decoder-only language model. You will feed it a database of names, and the model will generate new name ideas that all sound name-like, but are not already existing names. \n",
    "\n",
    "First, you'll train the model. After training, you'll generate names using pure random sampling as your decoding strategy. Pure random sampling doesn't always work well, so you'll also learn to tweak the temperature parameter when sampling, to better control your generation output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "In this section, you will take a look at the data. You should be familiar with it, but it is used a little different now that we are training a decoder model. Play with it and answer the questions at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "First 10 names: ['-maria carmen.', '-antonio.', '-maria.', '-manuel.', '-jose.']\n"
     ]
    }
   ],
   "source": [
    "data_filepath = \"../data/nombres_raw.txt\"  # Replace with your actual file path\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "start_token = \"-\"\n",
    "end_token = \".\"\n",
    "batch_size = 64\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "names = load_and_preprocess_data(data_filepath, alphabet, start_token, end_token)\n",
    "print(\"First 10 names:\", names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding names...\n",
      "First 10 encoded names: [[1, 16, 4, 21, 12, 4, 3, 6, 4, 21, 16, 8, 17, 2], [1, 4, 17, 23, 18, 17, 12, 18, 2], [1, 16, 4, 21, 12, 4, 2], [1, 16, 4, 17, 24, 8, 15, 2], [1, 13, 18, 22, 8, 2]]\n",
      "[22, 8, 21, 10, 12, 18]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = CharTokenizer(alphabet, start_token=start_token, end_token=end_token)\n",
    "\n",
    "# Encode names\n",
    "print(\"Encoding names...\")\n",
    "encoded_names = [tokenizer.encode(name) for name in names]\n",
    "print(\"First 10 encoded names:\", encoded_names[:5])\n",
    "\n",
    "print(tokenizer.encode('sergio'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the ```CharTokenizer``` to encode your name. What is the result of encoding your name?\n",
    ">>> Sergio\n",
    "\n",
    ">>> Write your name encoded\n",
    "[22, 8, 21, 10, 12, 18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item in batch:\n",
      "Input: tensor([ 1, 17, 18, 21, 16,  4,  3,  4, 21, 10,  8, 17, 23, 12, 17,  4,  0,  0,\n",
      "         0,  0])\n",
      "Target: tensor([17, 18, 21, 16,  4,  3,  4, 21, 10,  8, 17, 23, 12, 17,  4,  2,  0,  0,\n",
      "         0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = NameDataset(encoded_names)\n",
    "\n",
    "# Create data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Obtain a batch from data loader\n",
    "batch = next(iter(data_loader))\n",
    "\n",
    "# Let´s check one item from batch\n",
    "print(\"First item in batch:\")\n",
    "print(\"Input:\", batch[0][0])\n",
    "print(\"Target:\", batch[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can you obtain the target tensor from the input? Does this make sense for an autorregressive prediction such as the one of the Decoder-only model?\n",
    ">>> You shift all letters one step to the right, so the model learns to predict the next letter in the sequence. Yes, it makes sense.\n",
    "\n",
    "- What is the tensor value for the start token? And for the end token? And for the padding token?\n",
    ">>> Start token: 1, End token: 2, Padding token: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and tokenizer\n",
    "Here you will train the decoder model. Feel free to change the hyperparameters of the model in the ```model_params``` dictionary. Be careful with your computational resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [00:40<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 2.4030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 87.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Validation Loss: 2.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [00:23<00:00, 29.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 2.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 72.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Validation Loss: 1.9866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 380/698 [00:13<00:11, 27.93it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     23\u001b[39m val_loader = DataLoader(\n\u001b[32m     24\u001b[39m     val_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn=collate_fn\n\u001b[32m     25\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Call the train function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m model = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ICAI/4/NLPII/p2-decoder-sjrom47/notebook/../src/train.py:77\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, val_loader, tokenizer, num_epochs, learning_rate, model_save_dir, model_params, device)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[32m     76\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m optimizer.step()\n\u001b[32m     80\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/DL/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/DL/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/DL/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define training hyper parameters\n",
    "model_save_dir = \"runs\"\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "model_params = {\n",
    "    \"d_model\": 64,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 128,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"max_position_embeddings\": tokenizer.vocab_size # Do not touch this\n",
    "}\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Call the train function\n",
    "model = train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    model_save_dir=model_save_dir,\n",
    "    model_params=model_params,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Names\n",
    "Now we are ready to generate new names. Fill the function below and start playing around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix: str = \"\",\n",
    "    start_token: str = \"-\",\n",
    "    end_token: str = \".\",\n",
    "    max_length: int = 20,\n",
    "    temperature: float = 1.0,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a new name using the trained model, optionally starting with a given prefix.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        tokenizer (CharTokenizer): The tokenizer.\n",
    "        prefix (str): Optional prefix string to start the name.\n",
    "        start_token (str): The start token character.\n",
    "        end_token (str): The end token character.\n",
    "        max_length (int): Maximum length of the generated name (excluding prefix length).\n",
    "        temperature (float): Sampling temperature. Higher values increase randomness.\n",
    "        device (torch.device): Device to perform computation on.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated name.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_token_id = tokenizer.char2idx[start_token]\n",
    "    end_token_id = tokenizer.char2idx[end_token]\n",
    "\n",
    "    # TODO: Encode the prefix\n",
    "    prefix_ids = tokenizer.encode(prefix.lower()) if prefix else []\n",
    "\n",
    "    # TODO: Initialize the input with the start token and the prefix\n",
    "    generated_ids = [start_token_id] + prefix_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # TODO: Get model predictions\n",
    "            logits = model(torch.tensor(generated_ids).unsqueeze(0).to(device))\n",
    "            \n",
    "            # TODO: Apply softmax to get probabilities\n",
    "            next_token_probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "\n",
    "            # TODO: Sample the next token\n",
    "            next_token_id = torch.multinomial(next_token_probs, num_samples=1)\n",
    "\n",
    "            # TODO: Append the new token to the sequence\n",
    "            generated_ids.append(next_token_id.item())\n",
    "\n",
    "            # Stop if end token is generated\n",
    "            if next_token_id.item() == end_token_id:\n",
    "                break\n",
    "\n",
    "    # TODO: Decode the generated token IDs to a string, excluding start and end tokens\n",
    "    generated_sequence = tokenizer.decode(generated_ids)\n",
    "\n",
    "    # TODO: Decode the name\n",
    "    generated_name = generated_sequence\n",
    "\n",
    "    return generated_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_names = 5          # Number of names to generate\n",
    "max_length = 20         # Maximum length of each generated name\n",
    "temperature = 1.0       # Sampling temperature \n",
    "start_token = \"-\"       # Start token character (used during training)\n",
    "end_token = \".\"         # End token character (used during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names:\n",
      "\n",
      "-irmac.\n",
      "-darin.\n",
      "-luzsobet.\n",
      "-fermina.\n",
      "-alfonso victoro.\n"
     ]
    }
   ],
   "source": [
    "# Generate names\n",
    "print(\"Generated Names:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Temperature Parameter in Language Generation\n",
    "\n",
    "The **temperature** parameter $T$ adjusts the randomness of text generated by language models by scaling the logits (model outputs before softmax).\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Given logits $z_i$ for each token $i$, the probability $p_i$ of selecting token $i$ is calculated using the softmax function:\n",
    "\n",
    "\\begin{align*}\n",
    "p_i = \\frac{\\exp\\left(\\frac{z_i}{T}\\right)}{\\sum_{j} \\exp\\left(\\frac{z_j}{T}\\right)}\n",
    "\\end{align*}\n",
    "\n",
    "- *When $T = 1$*: The probabilities remain unchanged.\n",
    "- *When $T < 1$*: The distribution becomes sharper; higher-probability tokens are favored. You should expect names to look more like the \"typical\" names encountered in the dataset.\n",
    "- *When $T > 1$*: The distribution flattens; lower-probability tokens are more likely. You should expect names to look more \"exotic\" or \"creative\", since less probable characters are being sampled to continue the previously generated ones.\n",
    "\n",
    "### Impact on Token Probabilities\n",
    "\n",
    "Suppose we have logits for three tokens:\n",
    "\n",
    "- $z_A$ = 2.0\n",
    "- $z_B$ = 1.0\n",
    "- $z_C$ = 0.5\n",
    "\n",
    "##### At $T = 1.0$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.659\\\\\n",
    "p_B &= \\frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.242\\\\\n",
    "p_C &= \\frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.099\n",
    "\\end{align*}\n",
    "\n",
    "##### At $T = 0.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 0.5}}{e^{2.0 / 0.5} + e^{1.0 / 0.5} + e^{0.5 / 0.5}} = \\frac{e^{4.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.843\\\\\n",
    "p_B &= \\frac{e^{2.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.114\\\\\n",
    "p_C &= \\frac{e^{1.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.043\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Lower $T$ increases the dominance of the highest logit.\n",
    "\n",
    "##### At $T = 1.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 1.5}}{e^{2.0 / 1.5} + e^{1.0 / 1.5} + e^{0.5 / 1.5}} = \\frac{e^{1.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.490\\\\\n",
    "p_B &= \\frac{e^{0.667}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.282\\\\\n",
    "p_C &= \\frac{e^{0.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.228\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Higher $T$ increases the probabilities of less likely tokens.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- **Low Temperature ($T < 1$)**:\n",
    "  - **Sharper Distribution**: Model is confident; outputs are more predictable.\n",
    "  - **Use Case**: When coherence is crucial.\n",
    "\n",
    "- **High Temperature ($T > 1$)**:\n",
    "  - **Flatter Distribution**: Model explores more options; outputs are diverse.\n",
    "  - **Use Case**: When creativity is desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names with temperature=1.5:\n",
      "\n",
      "-fuizafota rwea.\n",
      "-nitolietl.\n",
      "-justibo.\n",
      "-rugindraca mariazomo\n",
      "-thyrine.\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.5  # High randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names with temperature=0.75:\n",
      "\n",
      "-hugo jordes.\n",
      "-lucas osmara.\n",
      "-deminia.\n",
      "-guanalldo.\n",
      "-julian frahur.\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.75  # Low randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-zhizand.\n",
      "-zouk.\n",
      "-zusane.\n",
      "-zighen.\n",
      "-zinnha.\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.0  # Default randomness\n",
    "prefix = \"Z\"  # Prefix to start the names with\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        prefix=prefix,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
